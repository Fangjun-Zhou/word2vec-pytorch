{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangjunzhou/Documents/ML_Project/word2vec-pytorch/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from train import train\n",
    "from utils.helper import (\n",
    "    get_model_class,\n",
    "    get_optimizer_class,\n",
    "    get_lr_scheduler,\n",
    "    save_vocab,\n",
    "    load_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config.yaml\"\n",
    "\n",
    "DATA_SET_SIZE = 1000\n",
    "\n",
    "PRE_TRAINED_MODEL_PATH = os.path.join(*[\"models\", \"skipgram_blog\", \"best_val_model_5.67.pt\"])\n",
    "PRE_TRAINED_VOCAB_PATH = os.path.join(*[\"models\", \"skipgram_blog\", \"vocab.pt\"])\n",
    "# PRE_TRAINED_MODEL_PATH = None\n",
    "# PRE_TRAINED_VOCAB_PATH = None\n",
    "\n",
    "VOCAB_MIN_WORD_FREQUENCY = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The corpus used for this training is [Twitter Financial News](https://www.kaggle.com/datasets/sulphatet/twitter-financial-news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Here are Thursday's biggest analyst calls: App...      0\n",
       "1  Buy Las Vegas Sands as travel to Singapore bui...      0\n",
       "2  Piper Sandler downgrades DocuSign to sell, cit...      0\n",
       "3  Analysts react to Tesla's latest earnings, bre...      0\n",
       "4  Netflix and its peers are set for a ‘return to...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the zip file.\n",
    "twitter_zip = zipfile.ZipFile(os.path.join(*[\"data\", \"twitter_financial_news.zip\"]))\n",
    "# Read the train_data and valid_data into dataframes.\n",
    "train_df = pd.read_csv(twitter_zip.open(\"train_data.csv\"))\n",
    "valid_df = pd.read_csv(twitter_zip.open(\"valid_data.csv\"))\n",
    "# Combine the dataframes.\n",
    "dataset_df = pd.concat([train_df, valid_df])\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for twitter financial news text.\n",
    "class TwitterFinancialNewsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, size = -1):\n",
    "        self.twitter_financial_news_df = df\n",
    "        # Shuffle and take a subset of the data.\n",
    "        if size > 0:\n",
    "            self.twitter_financial_news_df = self.twitter_financial_news_df.sample(frac=1).reset_index(drop=True)\n",
    "            self.twitter_financial_news_df = self.twitter_financial_news_df[:size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.twitter_financial_news_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.twitter_financial_news_df.iloc[idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datset.\n",
    "twitter_dataset = TwitterFinancialNewsDataset(dataset_df, size=DATA_SET_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained vocab size: 6630\n"
     ]
    }
   ],
   "source": [
    "if (PRE_TRAINED_VOCAB_PATH):\n",
    "    vocab: Vocab = load_vocab(PRE_TRAINED_VOCAB_PATH)\n",
    "    vocab_size = len(vocab.get_stoi())\n",
    "    print(f\"Pretrained vocab size: {vocab_size}\")\n",
    "else:\n",
    "    vocab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the english tokenizer.\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "# Build the extended vocab based on dataset.\n",
    "extend_vocab = build_vocab_from_iterator(\n",
    "    map(tokenizer, twitter_dataset),\n",
    "    min_freq=VOCAB_MIN_WORD_FREQUENCY\n",
    ")\n",
    "len(extend_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 new tokens added to the vocab.\n",
      "Extended vocab size: 6979\n"
     ]
    }
   ],
   "source": [
    "new_token = []\n",
    "for word in extend_vocab.get_stoi():\n",
    "    if not word in vocab:\n",
    "        new_token.append(word)\n",
    "# Add all new tokens to the vocab.\n",
    "for token in new_token:\n",
    "    vocab.append_token(token)\n",
    "print(f\"{len(new_token)} new tokens added to the vocab.\")\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Extended vocab size: {vocab_size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pretrained model.\n",
    "pretrained_model = torch.load(PRE_TRAINED_MODEL_PATH, map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size: 6979\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Epoch: 1/16, Train Loss=6.72336, Val Loss=6.28910\n",
      "Time elapsed: 0.18 min, average epoch time: 0.18 min, predicting finish time: 2.84 min\n",
      "Adjusting learning rate of group 0 to 2.3438e-02.\n",
      "Epoch: 2/16, Train Loss=5.69461, Val Loss=6.10489\n",
      "Time elapsed: 0.36 min, average epoch time: 0.18 min, predicting finish time: 2.84 min\n",
      "Adjusting learning rate of group 0 to 2.1875e-02.\n",
      "Epoch: 3/16, Train Loss=5.41644, Val Loss=5.96714\n",
      "Time elapsed: 0.53 min, average epoch time: 0.18 min, predicting finish time: 2.84 min\n",
      "Adjusting learning rate of group 0 to 2.0313e-02.\n",
      "Epoch: 4/16, Train Loss=5.18913, Val Loss=6.01272\n",
      "Time elapsed: 0.71 min, average epoch time: 0.18 min, predicting finish time: 2.83 min\n",
      "Adjusting learning rate of group 0 to 1.8750e-02.\n",
      "Epoch: 5/16, Train Loss=5.07249, Val Loss=5.94625\n",
      "Time elapsed: 0.89 min, average epoch time: 0.18 min, predicting finish time: 2.85 min\n",
      "Adjusting learning rate of group 0 to 1.7188e-02.\n",
      "Epoch: 6/16, Train Loss=4.94690, Val Loss=5.98138\n",
      "Time elapsed: 1.08 min, average epoch time: 0.18 min, predicting finish time: 2.87 min\n",
      "Adjusting learning rate of group 0 to 1.5625e-02.\n",
      "Epoch: 7/16, Train Loss=4.84946, Val Loss=5.93557\n",
      "Time elapsed: 1.27 min, average epoch time: 0.18 min, predicting finish time: 2.89 min\n",
      "Adjusting learning rate of group 0 to 1.4063e-02.\n",
      "Epoch: 8/16, Train Loss=4.76716, Val Loss=5.97280\n",
      "Time elapsed: 1.48 min, average epoch time: 0.19 min, predicting finish time: 2.97 min\n",
      "Adjusting learning rate of group 0 to 1.2500e-02.\n",
      "Epoch: 9/16, Train Loss=4.69811, Val Loss=5.96866\n",
      "Time elapsed: 1.72 min, average epoch time: 0.19 min, predicting finish time: 3.06 min\n",
      "Adjusting learning rate of group 0 to 1.0938e-02.\n",
      "Epoch: 10/16, Train Loss=4.62845, Val Loss=5.98210\n",
      "Time elapsed: 1.96 min, average epoch time: 0.20 min, predicting finish time: 3.13 min\n",
      "Adjusting learning rate of group 0 to 9.3750e-03.\n",
      "Epoch: 11/16, Train Loss=4.57119, Val Loss=5.97707\n",
      "Time elapsed: 2.20 min, average epoch time: 0.20 min, predicting finish time: 3.20 min\n",
      "Adjusting learning rate of group 0 to 7.8125e-03.\n",
      "Epoch: 12/16, Train Loss=4.50409, Val Loss=5.94728\n",
      "Time elapsed: 2.43 min, average epoch time: 0.20 min, predicting finish time: 3.24 min\n",
      "Adjusting learning rate of group 0 to 6.2500e-03.\n",
      "Epoch: 13/16, Train Loss=4.43822, Val Loss=5.97652\n",
      "Time elapsed: 2.64 min, average epoch time: 0.20 min, predicting finish time: 3.25 min\n",
      "Adjusting learning rate of group 0 to 4.6875e-03.\n",
      "Epoch: 14/16, Train Loss=4.37979, Val Loss=5.95766\n",
      "Time elapsed: 2.86 min, average epoch time: 0.20 min, predicting finish time: 3.27 min\n",
      "Adjusting learning rate of group 0 to 3.1250e-03.\n",
      "Epoch: 15/16, Train Loss=4.31360, Val Loss=5.96249\n",
      "Time elapsed: 3.06 min, average epoch time: 0.20 min, predicting finish time: 3.27 min\n",
      "Adjusting learning rate of group 0 to 1.5625e-03.\n",
      "Epoch: 16/16, Train Loss=4.25377, Val Loss=5.98748\n",
      "Time elapsed: 3.28 min, average epoch time: 0.21 min, predicting finish time: 3.28 min\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Training finished.\n",
      "Model artifacts saved to folder: models/skipgram_test\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    config=config,\n",
    "    data_iter=twitter_dataset,\n",
    "    vocab=vocab,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29a42755000d2da8e6ef3c423963b8e08bf5bcc98c62a722b0decb378a32dbbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
