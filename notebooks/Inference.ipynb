{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988b706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8c8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "\n",
    "import nltk\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.model import CBOW_Model\n",
    "from utils.dataset import CBOW_Dataset, SkipGram_Dataset\n",
    "from utils.constants import MIN_WORD_FREQUENCY, EMBED_DIMENSION\n",
    "from utils.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bd5ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'WikiText2',\n",
       " 'data_dir': 'data/',\n",
       " 'train_batch_size': 1000,\n",
       " 'val_batch_size': 1000,\n",
       " 'learning_rate': 0.025,\n",
       " 'epochs': 20,\n",
       " 'train_steps': None,\n",
       " 'val_steps': None,\n",
       " 'checkpoint_frequency': 5,\n",
       " 'model_dir': 'weights/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../config.yaml\", \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2d76dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../weights/cbow_model_010.pt\")\n",
    "vocab = torch.load(\"../weights/vocab.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c8341c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4122)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_raw = list(model.parameters())[0]\n",
    "embeddings_raw = embeddings_raw.cpu().detach().numpy()\n",
    "embeddings = embeddings_raw / ((embeddings_raw ** 2).sum(axis=0) ** (1 / 2))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e8a83e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father\n",
      "\n",
      "father: 1.000\n",
      "mother: 0.359\n",
      "husband: 0.350\n",
      "keamy: 0.346\n",
      "homer: 0.319\n",
      "childhood: 0.312\n",
      "successor: 0.312\n",
      "experiences: 0.309\n",
      "birth: 0.308\n",
      "relationship: 0.303\n"
     ]
    }
   ],
   "source": [
    "main_word_id = vocab[\"father\"]\n",
    "\n",
    "word_vec = embeddings[:, main_word_id]\n",
    "word_vec = np.reshape(word_vec, (1, len(word_vec)))\n",
    "\n",
    "dists = np.matmul(word_vec, embeddings).flatten()\n",
    "\n",
    "print(vocab.lookup_token(main_word_id))\n",
    "print()\n",
    "top5 = np.argsort(-dists)[:10]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc34ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father\n",
      "\n",
      "father: 1.000\n",
      "mother: 0.360\n",
      "wife: 0.315\n",
      "brother: 0.310\n",
      "son: 0.273\n",
      "parents: 0.242\n",
      "career: 0.239\n",
      "daughter: 0.233\n",
      "journey: 0.229\n",
      "friend: 0.213\n"
     ]
    }
   ],
   "source": [
    "main_word_id = vocab[\"father\"]\n",
    "\n",
    "word_vec = embeddings[:, main_word_id]\n",
    "word_vec = np.reshape(word_vec, (1, len(word_vec)))\n",
    "\n",
    "dists = np.matmul(word_vec, embeddings).flatten()\n",
    "\n",
    "print(vocab.lookup_token(main_word_id))\n",
    "print()\n",
    "top5 = np.argsort(-dists)[:10]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21e3231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do: 0.915\n",
      "leading: 0.817\n",
      "did: 0.455\n",
      "does: 0.428\n",
      "1939: 0.349\n"
     ]
    }
   ],
   "source": [
    "emb1 = embeddings[:, vocab[\"leading\"]]\n",
    "emb2 = embeddings[:, vocab[\"lead\"]]\n",
    "emb3 = embeddings[:, vocab[\"do\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "emb4 = np.reshape(emb4, (1, len(emb4)))\n",
    "dists = np.matmul(emb4, embeddings).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "557f10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 8.50MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_class = CBOW_Dataset\n",
    "\n",
    "train_dataset = dataset_class(\n",
    "    name=config[\"dataset\"],\n",
    "    set_type=\"train\",\n",
    "    data_dir=config[\"data_dir\"],\n",
    "    vocab=None,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76b0e792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9c468c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0e61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
