{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6852695",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9895037-4dab-4fe2-ac45-9885160f0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "from utils.model_config import ModelConfig\n",
    "from utils.project_config import ProjectConfig\n",
    "from utils.dataloader import Word2VecDataLoader\n",
    "from utils.model import ModelType\n",
    "from utils.model import CBOWModel, SkipGramModel\n",
    "from utils.trainer import Trainer\n",
    "from utils.helper import (\n",
    "    get_lr_scheduler,\n",
    "    save_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configs.\n",
    "project_config = ProjectConfig()\n",
    "project_config.use_cuda = True\n",
    "\n",
    "project_config.model_type = ModelType.SKIPGRAM\n",
    "project_config.criterion = nn.CrossEntropyLoss\n",
    "\n",
    "project_config.learning_rate = 0.025\n",
    "project_config.epochs = 8\n",
    "\n",
    "project_config.data_dir = \"data/\"\n",
    "project_config.model_dir = \"models/\"\n",
    "project_config.train_batch_size = 16\n",
    "project_config.val_batch_size = 16\n",
    "project_config.shuffle = True\n",
    "project_config.checkpoint_freq = 4\n",
    "\n",
    "word2vec_config = ModelConfig()\n",
    "word2vec_dataloader = Word2VecDataLoader(word2vec_config)\n",
    "# Check current device.\n",
    "if (project_config.use_cuda and torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Current Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a0e09",
   "metadata": {},
   "source": [
    "# Construct DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get english tokenizer.\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "# Get WikiText2 dataset.\n",
    "wiki_train, wiki_val, wiki_test = WikiText2()\n",
    "wiki_train = to_map_style_dataset(wiki_train)\n",
    "wiki_val = to_map_style_dataset(wiki_val)\n",
    "wiki_test = to_map_style_dataset(wiki_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of document length.\n",
    "doc_len = [len(doc) for doc in wiki_train]\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(doc_len, bins=list(range(1, 1000, 100)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2f165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build vocab from the dataset.\n",
    "vocab = word2vec_dataloader.build_vocab(wiki_train, tokenizer)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "# List first 10 vocabs.\n",
    "print(\"First 10 tokens: \", vocab.get_itos()[:10])\n",
    "# Encode an exist and non-existing token.\n",
    "# The default index for non-existing token is 0.\n",
    "print(\"Token id of [\\\"next\\\", \\\"fishball\\\"]: \", vocab([\"next\", \"fishball\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataLoader\n",
    "train_dataloader, vocab = word2vec_dataloader.get_dataloader_and_vocab(\n",
    "    model=project_config.model_type,\n",
    "    data_iter=wiki_train,\n",
    "    batch_size=project_config.train_batch_size,\n",
    "    shuffle=project_config.shuffle,\n",
    "    vocab=vocab\n",
    ")\n",
    "val_dataloader, _ = word2vec_dataloader.get_dataloader_and_vocab(\n",
    "    model=project_config.model_type,\n",
    "    data_iter=wiki_val,\n",
    "    batch_size=project_config.val_batch_size,\n",
    "    shuffle=project_config.shuffle,\n",
    "    vocab=vocab\n",
    ")\n",
    "print(f\"Training data size: {len(train_dataloader)}\")\n",
    "print(f\"Validation data size: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19cd242",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup.\n",
    "if project_config.model_type == ModelType.CBOW:\n",
    "    model = CBOWModel(vocab_size=vocab_size, config=word2vec_config)\n",
    "else:\n",
    "    model = SkipGramModel(vocab_size=vocab_size, config=word2vec_config)\n",
    "\n",
    "criterion = project_config.criterion()\n",
    "optimzier = optim.Adam(model.parameters(), lr=project_config.learning_rate)\n",
    "lr_scheduler = get_lr_scheduler(optimzier, project_config.epochs, verbose=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    epochs=project_config.epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    train_steps=project_config.train_steps,\n",
    "    val_dataloader=val_dataloader,\n",
    "    val_steps=project_config.val_steps,\n",
    "    checkpoint_frequency=project_config.checkpoint_freq,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimzier,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "    model_dir=project_config.model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.\n",
    "trainer.train()\n",
    "print(\"Train finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts.\n",
    "trainer.save_model()\n",
    "trainer.save_loss()\n",
    "save_vocab(vocab, project_config.model_dir)\n",
    "print(f\"Model artifacts saved to folder: {project_config.model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca12d48",
   "metadata": {},
   "source": [
    "# Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8982e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = trainer.loss[\"train\"]\n",
    "val_loss = trainer.loss[\"val\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss, label=\"train_loss\")\n",
    "ax.plot(val_loss, label=\"val_loss\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training and Validation Loss over Epochs\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
