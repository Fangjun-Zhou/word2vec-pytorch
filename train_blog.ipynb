{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(*[\"models\", \"skipgram\"])\n",
    "MODEL_NAME = \"skipgram\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "SHUFFLE = True\n",
    "DATA_SET_SIZE = 20000\n",
    "\n",
    "OPTIMIZER = \"Adam\"\n",
    "LEARNING_RATE = 0.030\n",
    "EPOCHS = 64\n",
    "TRAIN_STEPS = None\n",
    "VAL_STEPS = None\n",
    "\n",
    "CHECKPOINT_FREQUENCY = None\n",
    "\n",
    "# PRE_TRAINED_MODEL_PATH = os.path.join(*[\"weights\", \"skipgram_WikiText2\", \"model.pt\"])\n",
    "# PRE_TRAINED_VOCAB_PATH = os.path.join(*[\"weights\", \"skipgram_WikiText2\", \"vocab.pt\"])\n",
    "PRE_TRAINED_MODEL_PATH = None\n",
    "PRE_TRAINED_VOCAB_PATH = None\n",
    "\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.dataloader import get_custom_dataloader_and_vocab\n",
    "from utils.trainer import Trainer\n",
    "from utils.helper import (\n",
    "    get_model_class,\n",
    "    get_optimizer_class,\n",
    "    get_lr_scheduler,\n",
    "    save_vocab,\n",
    "    load_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current device.\n",
    "if (USE_CUDA and torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The corpus used for this training is [Blog Authorship Corpus\n",
    "](https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for blog text.\n",
    "class BlogDataset(Dataset):\n",
    "    def __init__(self, path, size = -1):\n",
    "        self.blog_df = pd.read_csv(path)\n",
    "        # Shuffle and take a subset of the data.\n",
    "        if size > 0:\n",
    "            self.blog_df = self.blog_df.sample(frac=1).reset_index(drop=True)\n",
    "            self.blog_df = self.blog_df[:size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.blog_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.blog_df.iloc[idx, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datset.\n",
    "blog_dataset = BlogDataset(os.path.join(*[\"data\", \"blog.zip\"]), size=DATA_SET_SIZE)\n",
    "# Split the dataset into train and validation sets.\n",
    "train_dataset, val_dataset = train_test_split(blog_dataset, test_size=0.2)\n",
    "# Get the size of train and validation sets.\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (PRE_TRAINED_VOCAB_PATH):\n",
    "    vocab = load_vocab(PRE_TRAINED_VOCAB_PATH)\n",
    "else:\n",
    "    vocab = None\n",
    "\n",
    "train_loader, vocab = get_custom_dataloader_and_vocab(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_iter=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    vocab=vocab\n",
    ")\n",
    "test_loader, _ = get_custom_dataloader_and_vocab(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_iter=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    vocab=vocab\n",
    ")\n",
    "\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = get_model_class(MODEL_NAME)\n",
    "model = model_class(vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_class = get_optimizer_class(OPTIMIZER)\n",
    "optimizer = optimizer_class(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_lr_scheduler(optimizer, EPOCHS, verbose=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    epochs=EPOCHS,\n",
    "    train_dataloader=train_loader,\n",
    "    train_steps=TRAIN_STEPS,\n",
    "    val_dataloader=test_loader,\n",
    "    val_steps=VAL_STEPS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    checkpoint_frequency=CHECKPOINT_FREQUENCY,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "    model_dir=MODEL_DIR,\n",
    "    model_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "if PRE_TRAINED_MODEL_PATH:\n",
    "    trainer.load_model(PRE_TRAINED_MODEL_PATH)\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.save_loss()\n",
    "save_vocab(vocab, MODEL_DIR)\n",
    "print(\"Model artifacts saved to folder:\", MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d4c59a2e37b723ec49a47066785740ed3807e59222da0467019684db6d44836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
