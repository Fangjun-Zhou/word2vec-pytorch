{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(*[\"models\", \"skipgram\"])\n",
    "MODEL_NAME = \"skipgram\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "SHUFFLE = True\n",
    "DATA_SET_SIZE = 10000\n",
    "\n",
    "OPTIMIZER = \"Adam\"\n",
    "LEARNING_RATE = 0.025\n",
    "EPOCHS = 32\n",
    "TRAIN_STEPS = None\n",
    "VAL_STEPS = None\n",
    "\n",
    "CHECKPOINT_FREQUENCY = None\n",
    "\n",
    "# PRE_TRAINED_MODEL_PATH = os.path.join(*[\"weights\", \"skipgram_WikiText2\", \"model.pt\"])\n",
    "# PRE_TRAINED_VOCAB_PATH = os.path.join(*[\"weights\", \"skipgram_WikiText2\", \"vocab.pt\"])\n",
    "PRE_TRAINED_MODEL_PATH = None\n",
    "PRE_TRAINED_VOCAB_PATH = None\n",
    "\n",
    "USE_CUDA = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The corpus used for this training is [Blog Authorship Corpus\n",
    "](https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine_Learning_Projects\\word2vec-pytorch\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.dataloader import get_custom_dataloader_and_vocab\n",
    "from utils.trainer import Trainer\n",
    "from utils.helper import (\n",
    "    get_model_class,\n",
    "    get_optimizer_class,\n",
    "    get_lr_scheduler,\n",
    "    save_vocab,\n",
    "    load_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for blog text.\n",
    "class BlogDataset(Dataset):\n",
    "    def __init__(self, path, size = -1):\n",
    "        self.blog_df = pd.read_csv(path)\n",
    "        # Shuffle and take a subset of the data.\n",
    "        if size > 0:\n",
    "            self.blog_df = self.blog_df.sample(frac=1).reset_index(drop=True)\n",
    "            self.blog_df = self.blog_df[:size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.blog_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.blog_df.iloc[idx, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the datset.\n",
    "blog_dataset = BlogDataset(os.path.join(*[\"data\", \"blog.zip\"]), size=DATA_SET_SIZE)\n",
    "# Split the dataset into train and validation sets.\n",
    "train_dataset, val_dataset = train_test_split(blog_dataset, test_size=0.2)\n",
    "# Get the size of train and validation sets.\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2562"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (PRE_TRAINED_VOCAB_PATH):\n",
    "    vocab = load_vocab(PRE_TRAINED_VOCAB_PATH)\n",
    "else:\n",
    "    vocab = None\n",
    "\n",
    "train_loader, vocab = get_custom_dataloader_and_vocab(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_iter=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    vocab=vocab\n",
    ")\n",
    "test_loader, _ = get_custom_dataloader_and_vocab(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_iter=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    vocab=vocab\n",
    ")\n",
    "\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current device.\n",
    "if (USE_CUDA and torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Epoch: 1/32, Train Loss=5.37179, Val Loss=5.28488\n",
      "Adjusting learning rate of group 0 to 2.4219e-02.\n",
      "Epoch: 2/32, Train Loss=5.30688, Val Loss=5.33356\n",
      "Adjusting learning rate of group 0 to 2.3438e-02.\n",
      "Epoch: 3/32, Train Loss=5.30013, Val Loss=5.28112\n",
      "Adjusting learning rate of group 0 to 2.2656e-02.\n",
      "Epoch: 4/32, Train Loss=5.30280, Val Loss=5.28806\n",
      "Adjusting learning rate of group 0 to 2.1875e-02.\n",
      "Epoch: 5/32, Train Loss=5.29809, Val Loss=5.27838\n",
      "Adjusting learning rate of group 0 to 2.1094e-02.\n",
      "Epoch: 6/32, Train Loss=5.29697, Val Loss=5.28052\n",
      "Adjusting learning rate of group 0 to 2.0313e-02.\n",
      "Epoch: 7/32, Train Loss=5.29381, Val Loss=5.28589\n",
      "Adjusting learning rate of group 0 to 1.9531e-02.\n",
      "Epoch: 8/32, Train Loss=5.29285, Val Loss=5.27907\n",
      "Adjusting learning rate of group 0 to 1.8750e-02.\n",
      "Epoch: 9/32, Train Loss=5.28747, Val Loss=5.26872\n",
      "Adjusting learning rate of group 0 to 1.7969e-02.\n",
      "Epoch: 10/32, Train Loss=5.28192, Val Loss=5.26569\n",
      "Adjusting learning rate of group 0 to 1.7188e-02.\n",
      "Epoch: 11/32, Train Loss=5.28227, Val Loss=5.26179\n",
      "Adjusting learning rate of group 0 to 1.6406e-02.\n",
      "Epoch: 12/32, Train Loss=5.27531, Val Loss=5.25741\n",
      "Adjusting learning rate of group 0 to 1.5625e-02.\n",
      "Epoch: 13/32, Train Loss=5.27307, Val Loss=5.25613\n",
      "Adjusting learning rate of group 0 to 1.4844e-02.\n",
      "Epoch: 14/32, Train Loss=5.26907, Val Loss=5.25555\n",
      "Adjusting learning rate of group 0 to 1.4063e-02.\n",
      "Epoch: 15/32, Train Loss=5.26960, Val Loss=5.26741\n",
      "Adjusting learning rate of group 0 to 1.3281e-02.\n",
      "Epoch: 16/32, Train Loss=5.25916, Val Loss=5.24336\n",
      "Adjusting learning rate of group 0 to 1.2500e-02.\n",
      "Epoch: 17/32, Train Loss=5.25358, Val Loss=5.24683\n",
      "Adjusting learning rate of group 0 to 1.1719e-02.\n",
      "Epoch: 18/32, Train Loss=5.25237, Val Loss=5.24453\n",
      "Adjusting learning rate of group 0 to 1.0938e-02.\n",
      "Epoch: 19/32, Train Loss=5.24908, Val Loss=5.24344\n",
      "Adjusting learning rate of group 0 to 1.0156e-02.\n",
      "Epoch: 20/32, Train Loss=5.24138, Val Loss=5.24067\n",
      "Adjusting learning rate of group 0 to 9.3750e-03.\n",
      "Epoch: 21/32, Train Loss=5.23833, Val Loss=5.24244\n",
      "Adjusting learning rate of group 0 to 8.5938e-03.\n",
      "Epoch: 22/32, Train Loss=5.23374, Val Loss=5.23119\n",
      "Adjusting learning rate of group 0 to 7.8125e-03.\n",
      "Epoch: 23/32, Train Loss=5.22744, Val Loss=5.22335\n",
      "Adjusting learning rate of group 0 to 7.0313e-03.\n",
      "Epoch: 24/32, Train Loss=5.22535, Val Loss=5.21645\n",
      "Adjusting learning rate of group 0 to 6.2500e-03.\n",
      "Epoch: 25/32, Train Loss=5.21271, Val Loss=5.21531\n",
      "Adjusting learning rate of group 0 to 5.4688e-03.\n",
      "Epoch: 26/32, Train Loss=5.21188, Val Loss=5.20446\n",
      "Adjusting learning rate of group 0 to 4.6875e-03.\n",
      "Epoch: 27/32, Train Loss=5.20206, Val Loss=5.20640\n",
      "Adjusting learning rate of group 0 to 3.9062e-03.\n",
      "Epoch: 28/32, Train Loss=5.19600, Val Loss=5.20445\n",
      "Adjusting learning rate of group 0 to 3.1250e-03.\n",
      "Epoch: 29/32, Train Loss=5.18087, Val Loss=5.19944\n",
      "Adjusting learning rate of group 0 to 2.3438e-03.\n",
      "Epoch: 30/32, Train Loss=5.17221, Val Loss=5.19089\n",
      "Adjusting learning rate of group 0 to 1.5625e-03.\n",
      "Epoch: 31/32, Train Loss=5.15805, Val Loss=5.19224\n",
      "Adjusting learning rate of group 0 to 7.8125e-04.\n",
      "Epoch: 32/32, Train Loss=5.14172, Val Loss=5.18410\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Training finished.\n",
      "Model artifacts saved to folder: models\\skipgram\n"
     ]
    }
   ],
   "source": [
    "model_class = get_model_class(MODEL_NAME)\n",
    "model = model_class(vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_class = get_optimizer_class(OPTIMIZER)\n",
    "optimizer = optimizer_class(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_lr_scheduler(optimizer, EPOCHS, verbose=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    epochs=EPOCHS,\n",
    "    train_dataloader=train_loader,\n",
    "    train_steps=TRAIN_STEPS,\n",
    "    val_dataloader=test_loader,\n",
    "    val_steps=VAL_STEPS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    checkpoint_frequency=CHECKPOINT_FREQUENCY,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "    model_dir=MODEL_DIR,\n",
    "    model_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "if PRE_TRAINED_MODEL_PATH:\n",
    "    trainer.load_model(PRE_TRAINED_MODEL_PATH)\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.save_loss()\n",
    "save_vocab(vocab, MODEL_DIR)\n",
    "print(\"Model artifacts saved to folder:\", MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d4c59a2e37b723ec49a47066785740ed3807e59222da0467019684db6d44836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
